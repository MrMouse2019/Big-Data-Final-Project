module load spark/2.2.0

spark-submit --conf spark.pyspark.python=/share/apps/python/3.6.5/bin/python task1_test.py /user/hm74/NYCOpenData/2abb-gr8d.tsv.gz
-----------------------------------------------------------------------------------------------------------------------------------------------------
from csv import reader
lines = sc.textFile("/user/hm74/NYCOpenData/2abb-gr8d.tsv.gz")
lines = lines.mapPartitions(lambda x: reader(x, delimiter='\t'))
headerArray = lines.first()
num_columns = len(headerArray)

from itertools import islice
lines = lines.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)
-------------------------------------------------------------------------------------------
column0 = lines.map(lambda x: x[0])

df = spark.read.csv("/user/hm74/NYCOpenData/2abb-gr8d.tsv.gz", sep='\t', header='true')
df.createOrReplaceTempView("df")

type0 = str(df.schema['Week'].dataType)
from pyspark.sql.types import *
isinstance(df.schema["Week"].dataType, StringType)

filtered_column0 = column0.filter(lambda x: type(x) is str)

# count
count0 = filtered_column0.count()

# shortest
shortest_values = filtered_column0.map(lambda x: (x, len(x))).sortBy(lambda x: x[1]).map(lambda x:x[0]).take(5)

# longest
longest_values = filtered_column0.map(lambda x: (x, len(x))).sortBy(lambda x: x[1], ascending=False).map(lambda x:x[0]).take(5)

from operator import add
# average length
total_length = filtered_column0.map(lambda x: len(x)).reduce(add)
average_length = total_length / count0



